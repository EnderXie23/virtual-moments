{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main file to invoke My Virtual Moments application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare the LLM for question answering.\n",
    "In this naive implementation, we want to first ensure that llama3 (or any other possible models) may respond to user requests well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d1c889e59f45c5882a386846ce5a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /ssdshare/LLMs/llama3-Chinese-chat-8b/ and are newly initialized: ['model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We want to load the model first\n",
    "import accelerate, bitsandbytes\n",
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "model_path = '/ssdshare/LLMs/llama3-Chinese-chat-8b/'\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path,padding_side='left')\n",
    "qconfig=BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             quantization_config=qconfig) \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function to get answers from the LLM\n",
    "def chat(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096)\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def question_prompt(question):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\" Please be a helpful assistant and answer the following question:\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n",
    "    ]\n",
    "    prompt = \"\"\n",
    "    for message in chat:\n",
    "        prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return prompt\n",
    "\n",
    "def chat_with_llm(question):\n",
    "    prompt = question_prompt(question)\n",
    "    return chat(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system:  Please be a helpful assistant and answer the following question:\n",
      "user: Question: What is the capital of France?\n",
      "assistant: The capital of France is called Paris.\n"
     ]
    }
   ],
   "source": [
    "# Utilize the functions defined above to chat with the model\n",
    "print(chat_with_llm(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "class LocalLlama:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\",  padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a transformer pipeline\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = LocalLlama()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    device_map = \"cuda:0\",\n",
    "    max_length = 4096,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Tell me your name. What's your name? Are you hungry? Are you tired? Do you have a job? Do you like dogs? Are you looking for a pet? Do you like cats? Are you allergic to cats? Are you allergic to dogs? Do you like to travel? Have you been to Europe? Have you been to Asia? Have you been to South America? Have you been to Africa? Have you been to Australia? Have you been to New Zealand? Have you been to Antarctica?\n"
     ]
    }
   ],
   "source": [
    "# Test if the llm chain works properly\n",
    "input_text = \"Hi! Tell me your name.\"\n",
    "print(llm.predict(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implement few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "def load_qa_pairs(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    # Ensuring the file has an even number of lines for perfect pairing\n",
    "    assert len(lines) % 2 == 0, \"The text file should contain an even number of lines.\"\n",
    "\n",
    "    # Pairing questions and answers\n",
    "    qa_pairs = []\n",
    "    for i in range(0, len(lines), 2):\n",
    "        question = lines[i]\n",
    "        answer = lines[i+1]\n",
    "        qa_pairs.append(f\"Question: {question} Answer: {answer}\")\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def prepare_training_data(qa_pairs, block_size=256):\n",
    "    # Concatenate QA pairs until the block size is reached\n",
    "    training_instances = []\n",
    "    current_block = \"\"\n",
    "    for pair in qa_pairs:\n",
    "        if len(current_block) + len(pair) + 1 > block_size:\n",
    "            training_instances.append(current_block.strip())\n",
    "            current_block = \"\"\n",
    "        current_block += pair + \" \"\n",
    "    if current_block:\n",
    "        training_instances.append(current_block.strip())\n",
    "    return training_instances\n",
    "\n",
    "def tokenize_data(tokenizer, text_data, max_length=256):\n",
    "    return tokenizer(text_data, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized training data\n",
    "raw_data = load_qa_pairs(\"data/furina.txt\")\n",
    "encodings = tokenize_data(tokenizer, prepare_training_data(raw_data))\n",
    "tokenized_data = QADataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedLlama:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fine_tuned_model = None\n",
    "\n",
    "        # Create a data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer = self.tokenizer,\n",
    "            mlm = False,\n",
    "        )\n",
    "        # Define the training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = \"./llama_fine_tuned\",\n",
    "            overwrite_output_dir = True,\n",
    "            num_train_epochs = 3,\n",
    "            per_device_train_batch_size = 2,\n",
    "            save_steps = 1000,\n",
    "            save_total_limit = 2,\n",
    "            logging_dir = \"./logs\",\n",
    "        )\n",
    "\n",
    "        # Create the trainer\n",
    "        trainer = Trainer(\n",
    "            model = self.model,\n",
    "            args = training_args,\n",
    "            data_collator = data_collator,\n",
    "            train_dataset = tokenized_data,\n",
    "        )\n",
    "        trainer.train()\n",
    "        self.fine_tuned_model = trainer.model\n",
    "        print(\"Fine tuning completed.\")\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        if self.fine_tuned_model:\n",
    "            print(\"Using the fine tuned model.\")\n",
    "            outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        else:\n",
    "            print(\"The model is not fine tuned yet. Using the original model.\")\n",
    "            outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftllm = FineTunedLlama()\n",
    "print(ftllm.predict(\"你好！你现在有时间吗？\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trying to do few-shot learning using conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversation_context(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    assert len(lines) % 2 == 0, \"Each question should have a corresponding answer.\"\n",
    "\n",
    "    conversation = \"\"\n",
    "    for i in range(0, len(lines), 2):\n",
    "        question = lines[i]\n",
    "        answer = lines[i+1]\n",
    "        conversation += f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "class ConversationLlama:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, input_text, conversation_context):\n",
    "        # Add the new question to the ongoing conversation context\n",
    "        full_text = conversation_context + f\"Question: {input_text}\\n\"\n",
    "\n",
    "        # Encode the text input to tensor\n",
    "        input_ids = self.tokenizer(full_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids, max_new_tokens=1000, pad_token_id=self.tokenizer.eos_token_id, do_sample=True, temperature=0.7)\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What would you say when meeting for the first time?\n",
      "Answer: Why are you just standing there with your mouth gaping? Ah, you must be stunned and at a loss for words... Understandable, it is I after all... Fontaine's most beloved star, Furina. I'm on a very tight schedule, so you're lucky to even get an appointment with me.\n",
      "Question: What would you say when chatting about tea parties?\n",
      "Answer: Tea parties are a must for the well-mannered. If you'd like to learn the proper etiquette, I'd be happy to teach you.\n",
      "Question: What would you say when bored?\n",
      "Answer: Boring... Isn't there anything else more interesting to do?\n",
      "Question: What would you say when being popular?\n",
      "Answer: *sigh* Being too popular can be such a hassle. Who knew the people would adore me so much?\n",
      "Question: What would you say when it's raining?\n",
      "Answer: It's pouring out here! Wait, the water levels aren't rising, are they?\n",
      "Question: What would you say when it's snowing?\n",
      "Answer: Wow, this would be the perfect location for shooting a film.\n",
      "Question: What would you say when it's sunny?\n",
      "Answer: The sun feels almost as hot as those studio lights I'm in front of all the time... Good thing there can only be one sun.\n",
      "Question: What would you say when it's windy?\n",
      "Answer: \"Listen, there is a disturbing growling in the wind. That's the sound of a long-forgotten sea monster having a nightmare.\" Huh? Why is it having a nightmare? Uh... I dunno, because fear has to manifest itself somehow?\n",
      "Question: What would you say when you're in the desert?\n",
      "Answer: What a wild and desolate sight... Allow me to grant you the blessing of water!\n",
      "Question: What would you say in the morning?\n",
      "Answer: Good morning... Must I really get up this early? Just let me sleep a little longer...\n",
      "Question: What would you say at noon?\n",
      "Answer: Good afternoon! Where's my cake? What? I just had cake for breakfast? Well, I think it's been long enough!\n",
      "Question: What would you say  in the evening?\n",
      "Answer: Good evening. *sigh* Mademoiselle Crabaletta has been muttering about getting in shape lately and even said that she wanted to drag me along with her. Hmph, I'm already eating much healthier than last month. I work very hard to maintain my figure. Hey, you can tell, can't you!?\n",
      "Question: What would you say before going to bed?\n",
      "Answer: Whew, I'm getting sleepy myself. See you tomorrow. Remember to wake me up on time...\n",
      "Question: What would you say when chatting about followers?\n",
      "Answer: Hmm? Who are these people at the tea party? Why, they are my loyal followers of course! *sigh* Alright, I suppose I'll introduce them to you. This is Mademoiselle Crabaletta, my most adorable maid... although she accidentally cuts my outfits from time to time... This is Surintendante Chevalmarin, my reliable housekeeper who keeps my busy life in order. And finally, the one keeping everybody in line is Gentilhomme Usher, my loyal conferencier. Hehe, I may have allowed you to participate in my tea party, but you'll still need to work hard to gain their approval!\n",
      "Question: What would you say about singing?\n",
      "Answer: I'm very confident in my singing skills, but there aren't many pieces of music that are worthy of my vocal prowess. I hope the creatives in the theater troupe get their act together and don't keep me waiting in vain...\n",
      "Question: What would you say about us?\n",
      "Answer: *sigh* Given that we know each other, you may relax a little and needn't act so respectfully in my presence. Wait, what's that expression on your face? Don't tell me that you've never respected me from the very beginning!?\n",
      "Question: What would you say about us travelling together?\n",
      "Answer: My story has already come to an end, and so the next act shall be about OUR story... In which case, we should probably start charging double for public appearances... Oh, I'm so happy!\n",
      "Question: What would you say about the god's eye?\n",
      "Answer: Once the ancient prophecy came to an end and everything was over, I fell into low spirits for a very long time. People who stand on the stage basking in the adulation of their audience must also bear the pressures of being in the public eye and living up to everyone's expectations. But I knew very well my person wouldn't be enough, and that putting on the act of a god would be able the only way to satisfy their adoration... And at the end of the day, all I ever experienced was loneliness. So there came a point when I loathed the very thought of acting and locked myself in my room. It wasn't until the moment I stood on stage and faced the audience again that I realized the anxiety in my heart had dissipated. The reason I can now stand before the crowd's watchful gaze is perhaps because... I have finally started to act as myself.\n",
      "Question: What would you say about operas?\n",
      "Answer: Fontaine's opera house has seen several reforms, and with each reform, opposition was to be expected. As I remember, crowds would often gather in the opera house making a ruckus... That one time, it became so loud my ears were ringing. I frightened them by stating, \"I forbid anyone from discussing such pointless matters!\", I clearly remember Neuvillette tapping his cane on the ground at that precise moment while the whole place immediately fell silent. Sometimes I can't help but inadvertently show my intimidating side... I hope my people understands.\n",
      "Question: What would you say about Lumitoiles?\n",
      "Answer: I really like the Lumitoile. Those creatures can glow and shine even in places where no one sees them... That's what I call a true \"star.\" Ah, if only there were an opera about the Lumitoile, then I could literally play the role of a lovely rogue starfish on stage.\n",
      "Question: What would you like to do today?\n",
      " extensively at all, for the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cllm = ConversationLlama(model, tokenizer)\n",
    "\n",
    "# Load the conversation context\n",
    "conversation_context = load_conversation_context(\"data/furina2.txt\")\n",
    "\n",
    "# Making a prediction with the new question\n",
    "new_question = \"What would you like to do today?\"\n",
    "response = cllm.predict(new_question, conversation_context)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
