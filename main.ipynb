{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main file to invoke My Virtual Moments application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare the LLM for question answering.\n",
    "In this naive implementation, we want to first ensure that llama3 (or any other possible models) may respond to user requests well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ef123c18b344beb2638094848eba9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We want to load the model first\n",
    "import accelerate, bitsandbytes\n",
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "model_path = '/ssdshare/LLMs/llama3-Chinese-chat-8b/'\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path,padding_side='left')\n",
    "qconfig=BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=qconfig) \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function to get answers from the LLM\n",
    "def chat(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096)\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def question_prompt(question):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\" Please be a helpful assistant and answer the following question:\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n",
    "    ]\n",
    "    prompt = \"\"\n",
    "    for message in chat:\n",
    "        prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return prompt\n",
    "\n",
    "def chat_with_llm(question):\n",
    "    prompt = question_prompt(question)\n",
    "    return chat(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system:  Please be a helpful assistant and answer the following question:\n",
      "user: Question: What is the capital of France?\n",
      "system: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Utilize the functions defined above to chat with the model\n",
    "print(chat_with_llm(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "class LocalLlama:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\",  padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a transformer pipeline\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = LocalLlama()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    device_map = \"cuda:0\",\n",
    "    max_length = 4096,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Tell me your name. What's your name? Are you hungry? Are you tired? Do you have a job? Do you like dogs? Are you looking for a pet? Do you like cats? Are you allergic to cats? Are you allergic to dogs? Do you like to travel? Have you been to Europe? Have you been to Asia? Have you been to South America? Have you been to Africa? Have you been to Australia? Have you been to New Zealand? Have you been to Antarctica?\n"
     ]
    }
   ],
   "source": [
    "# Test if the llm chain works properly\n",
    "input_text = \"Hi! Tell me your name.\"\n",
    "print(llm.predict(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implement few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "def load_qa_pairs(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    # Ensuring the file has an even number of lines for perfect pairing\n",
    "    assert len(lines) % 2 == 0, \"The text file should contain an even number of lines.\"\n",
    "\n",
    "    # Pairing questions and answers\n",
    "    qa_pairs = []\n",
    "    for i in range(0, len(lines), 2):\n",
    "        question = lines[i]\n",
    "        answer = lines[i+1]\n",
    "        qa_pairs.append(f\"Question: {question} Answer: {answer}\")\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def prepare_training_data(qa_pairs, block_size=256):\n",
    "    # Concatenate QA pairs until the block size is reached\n",
    "    training_instances = []\n",
    "    current_block = \"\"\n",
    "    for pair in qa_pairs:\n",
    "        if len(current_block) + len(pair) + 1 > block_size:\n",
    "            training_instances.append(current_block.strip())\n",
    "            current_block = \"\"\n",
    "        current_block += pair + \" \"\n",
    "    if current_block:\n",
    "        training_instances.append(current_block.strip())\n",
    "    return training_instances\n",
    "\n",
    "def tokenize_data(tokenizer, text_data, max_length=256):\n",
    "    return tokenizer(text_data, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenized training data\n",
    "raw_data = load_qa_pairs(\"data/furina2.txt\")\n",
    "encodings = tokenize_data(tokenizer, prepare_training_data(raw_data))\n",
    "tokenized_data = QADataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedLlama:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fine_tuned_model = None\n",
    "\n",
    "        # Create a data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer = self.tokenizer,\n",
    "            mlm = False,\n",
    "        )\n",
    "        # Define the training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = \"./llama_fine_tuned\",\n",
    "            overwrite_output_dir = True,\n",
    "            num_train_epochs = 3,\n",
    "            per_device_train_batch_size = 2,\n",
    "            save_steps = 1000,\n",
    "            save_total_limit = 2,\n",
    "            logging_dir = \"./logs\",\n",
    "        )\n",
    "\n",
    "        # Create the trainer\n",
    "        trainer = Trainer(\n",
    "            model = self.model,\n",
    "            args = training_args,\n",
    "            data_collator = data_collator,\n",
    "            train_dataset = tokenized_data,\n",
    "        )\n",
    "        trainer.train()\n",
    "        self.fine_tuned_model = trainer.model\n",
    "        print(\"Fine tuning completed.\")\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        if self.fine_tuned_model:\n",
    "            print(\"Using the fine tuned model.\")\n",
    "            outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        else:\n",
    "            print(\"The model is not fine tuned yet. Using the original model.\")\n",
    "            outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftllm = FineTunedLlama()\n",
    "print(ftllm.predict(\"Hi!Are you free now?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trying to do few-shot learning using conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversation_context(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    assert len(lines) % 2 == 0, \"Each question should have a corresponding answer.\"\n",
    "\n",
    "    conversation = \"\"\n",
    "    for i in range(0, 14, 2):\n",
    "        question = lines[i]\n",
    "        answer = lines[i+1]\n",
    "        conversation += f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "class ConversationLlama:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, input_text, conversation_context):\n",
    "        # Add the new question to the ongoing conversation context\n",
    "        full_text = \"Do a role play and learn from the following QA examples:\" + conversation_context + f\"Now answer the given qeustion in similar tone: {input_text}\\n\"\n",
    "\n",
    "        # Encode the text input to tensor\n",
    "        input_ids = self.tokenizer(full_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids, max_new_tokens=1000, pad_token_id=self.tokenizer.eos_token_id, do_sample=True, temperature=0.7)\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cllm = ConversationLlama(model, tokenizer)\n",
    "\n",
    "# Load the conversation context\n",
    "conversation_context = load_conversation_context(\"data/furina2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do a role play and learn from the following QA examples:Question: What would you say when meeting for the first time?\n",
      "Answer: Why are you just standing there with your mouth gaping? Ah, you must be stunned and at a loss for words... Understandable, it is I after all... Fontaine's most beloved star, Furina. I'm on a very tight schedule, so you're lucky to even get an appointment with me.\n",
      "Question: What would you say when chatting about tea parties?\n",
      "Answer: Tea parties are a must for the well-mannered. If you'd like to learn the proper etiquette, I'd be happy to teach you.\n",
      "Question: What would you say when bored?\n",
      "Answer: Boring... Isn't there anything else more interesting to do?\n",
      "Question: What would you say when being popular?\n",
      "Answer: *sigh* Being too popular can be such a hassle. Who knew the people would adore me so much?\n",
      "Question: What would you say when it's raining?\n",
      "Answer: It's pouring out here! Wait, the water levels aren't rising, are they?\n",
      "Question: What would you say when it's snowing?\n",
      "Answer: Wow, this would be the perfect location for shooting a film.\n",
      "Question: What would you say when it's sunny?\n",
      "Answer: The sun feels almost as hot as those studio lights I'm in front of all the time... Good thing there can only be one sun.\n",
      "Now answer the given qeustion in similar tone: If you adopted a beatiful dog, what would you say?\n",
      "Answer: I'm happy to have such a wonderful companion!\n"
     ]
    }
   ],
   "source": [
    "new_question = \"If you adopted a beatiful dog, what would you say?\"\n",
    "response = cllm.predict(new_question, conversation_context)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
