{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main file to invoke My Virtual Moments application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare the LLM for question answering.\n",
    "In this naive implementation, we want to first ensure that llama3 (or any other possible models) may respond to user requests well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53440e1d502d455098f773d413539e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# We want to load the model first\n",
    "import accelerate, bitsandbytes\n",
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "model_path = '/ssdshare/LLMs/llama3-Chinese-chat-8b/'\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path,padding_side='left')\n",
    "qconfig=BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map=\"cuda:0\", \n",
    "                                             quantization_config=qconfig) \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function to get answers from the LLM\n",
    "def chat(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096)\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def question_prompt(question):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\" Please be a helpful assistant and answer the following question:\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n",
    "    ]\n",
    "    s = tokenizer.apply_chat_template(chat, tokenize = False)\n",
    "    return s\n",
    "\n",
    "def chat_with_llm(question):\n",
    "    prompt = question_prompt(question)\n",
    "    return chat(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Please be a helpful assistant and answer the following question:user\n",
      "\n",
      "Question: What is the capital of France?assistant\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Utilize the functions defined above to chat with the model\n",
    "print(chat_with_llm(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the langchain pipeline with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLlama:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, input_text):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\",  padding=True, truncation=True, max_length=4096).input_ids.to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a transformer pipeline\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = LocalLlama()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    device_map = \"cuda:0\",\n",
    "    max_length = 4096,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Tell me your name. I'm Sarah.\n",
      "Hi! My name is Sarah. I'm a writer, editor and creative living in Durham, North Carolina.\n",
      "I am a professional writer with 5+ years of experience. I have a background in journalism, having worked at local and national publications. I have experience in a variety of writing styles, including news, features, and opinion pieces. I am skilled in interviewing, researching, and writing compelling stories.\n",
      "I have a degree in journalism from the University of North Carolina at Chapel Hill.\n",
      "I am currently a freelance writer and editor.\n",
      "I am available for work on a freelance basis.\n",
      "I have a wide range of skills that I can offer to clients, including writing, editing, and proofreading.\n",
      "I am available for work on a freelance basis and can provide samples of my work upon request.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hi! Tell me your name.\"\n",
    "print(llm.predict(input_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
